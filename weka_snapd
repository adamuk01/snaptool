#!/usr/bin/env python3

# Weka Snapshot Management Daemon
# Vince Fleming
# vince@weka.io
#
VERSION="0.1.0"

# system imports
import logging
import logging.handlers
from logging import debug, info, warning, error, critical
import argparse
import os
import sys
import time
import traceback
from multiprocessing import Process
import threading
import json
import yaml
import datetime
from urllib3 import add_stderr_logger
import queue

from wekacluster import WekaCluster
import signals
from snapshots import SnapSchedule, MonthlySchedule, WeeklySchedule, DailySchedule, HourlySchedule
from upload import UploadSnapshot, replay_upload_intent_log

def snapd_call_api(cluster, method, parms):
    stopit = False
    num_errors = 0
    while not stopit:
        # api only fails under extreme circumstances
        try:
            api_return = cluster.call_api(method=method,parms=parms)
            stopit = True
        #except APIException as exc:
        except Exception as exc:
            num_errors += 1
            logger.error(f"Error {exc} while executing API method {method}; attemping to re-establish communications (retry {num_errors})")
            if num_errors > 10: # give up
                raise
            time.sleep(5)               # give the cluster some time to resolve it's issues
            cluster.refresh_config()    # will try to re-establish communications
    return api_return

if __name__ == '__main__':
    # handle signals (ie: ^C and such)
    signals.signal_handling()

    parser = argparse.ArgumentParser(description="Weka Snapshot Management Daemon")
    parser.add_argument("-c", "--configfile", dest='configfile', default="./weka_snapd.yml", help="override ./weka_snapd.yml as config file")
    parser.add_argument("-p", "--port", dest='port', default="13999", help="TCP port number to listen on")
    parser.add_argument( 'clusterspec', default="localhost", help="Cluster specification.  <host>,<host>,...:authfile" )
    parser.add_argument("-v", "--verbosity", action="count", default=0, help="increase output verbosity")
    parser.add_argument("--version", dest="version", default=False, action="store_true", help="Display version number")
    args = parser.parse_args()

    if args.version:
        print( f"{sys.argv[0]} version {VERSION}" )
        sys.exit( 0 )

    if args.verbosity == 0:
        loglevel = logging.ERROR
    elif args.verbosity == 1:
        loglevel = logging.WARNING
    elif args.verbosity == 2:
        loglevel = logging.INFO
    elif args.verbosity > 2:
        loglevel = logging.DEBUG

    # set the root logger
    logger = logging.getLogger()
    FORMAT = "%(process)s:%(filename)s:%(lineno)s:%(funcName)s():%(levelname)s:%(message)s"
    #logging.basicConfig(format=FORMAT)
    logger.setLevel(loglevel)

    # create handler to log to syslog
    syslog_handler = logging.handlers.SysLogHandler(address="/dev/log")
    #syslog_handler.setLevel(loglevel)
    #syslog_handler.setFormatter(logging.Formatter(os.path.basename( sys.argv[0] ) + ': %(levelname)s: %(message)s'))
    syslog_handler.setFormatter(logging.Formatter(FORMAT))

    # create handler to log to stderr
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(logging.Formatter(FORMAT))
    #console_handler.setLevel(loglevel)

    # add handlers to root logger
    logger.addHandler(syslog_handler)
    logger.addHandler(console_handler)

    # configure logging in signals module
    #sublog = logging.getLogger( "signals" )
    #sublog.setLevel(loglevel)

    # configure logging in wekacluster module
    sublog = logging.getLogger( "wekacluster" )
    sublog.setLevel(logging.INFO)

    # configure logging in wekaapi module
    sublog = logging.getLogger( "wekaapi" )
    sublog.setLevel(logging.DEBUG)

    # configure logging in sthreads module
    sublog = logging.getLogger( "sthreads" )
    sublog.setLevel(logging.ERROR)

    # configure logging in circular module
    logging.getLogger("circular").setLevel(logging.INFO)

    # configure logging in snapshots module
    logging.getLogger("snapshots").setLevel(logging.INFO)

    # configure logging in upload module
    logging.getLogger("upload").setLevel(logging.DEBUG)

    # configure logging in connectionpool module
    #logging.getLogger("connectionpool").setLevel(logging.ERROR)
    #logging.getLogger("retry").setLevel(logging.ERROR)
    # urllib3 logging
    add_stderr_logger(level=logging.ERROR)

    # configuration JRPC API
    #api_thread = threading.Thread(target=start_api, args=(args.port, True))

    with open(args.configfile, 'r') as f:
        config = yaml.load(stream=f)

    #logger.debug(f"{json.dumps(config)}")

    #
    # Sanity-check configuration?
    #


    #subprocesses = {}
    #port = int(args.port)

    # create Cluster object so we can use the Weka API
    clusterspeclist = args.clusterspec.split(":")
    if len(clusterspeclist) > 1:
        cluster_auth = clusterspeclist[1]
    else:
        cluster_auth = None

    cluster_obj = WekaCluster(clusterspeclist[0], cluster_auth)

    logging.info(f"{sys.argv[0]} starting")

    # configure snap schedules
    schedules = {}
    monthly = MonthlySchedule()
    weekly = WeeklySchedule()
    daily = DailySchedule()
    hourly = HourlySchedule()
    schedules["default"] = SnapSchedule("default", monthly, weekly, daily, hourly) # defalt schedule, always there

    # custom schedules
    config_schedules = config["schedules"]
    for schedname, sched in config_schedules.items():
        #logger.debug(f"schedname={schedname}, sched={sched}")
        monthly = MonthlySchedule(date=sched['monthly']['date'], time=sched['monthly']['time'], retain=sched['monthly']['retain'])
        weekly = WeeklySchedule(weekday=sched['weekly']['weekday'], time=sched['weekly']['time'], retain=sched['weekly']['retain'])
        daily = DailySchedule(time=sched['daily']['time'], start_day=sched['daily']['start_day'], stop_day=sched['daily']['stop_day'], retain=sched['daily']['retain'])
        hourly = HourlySchedule(start_day=sched['hourly']['start_day'], stop_day=sched['hourly']['stop_day'], start_time=sched['hourly']['start_time'], stop_time=sched['hourly']['stop_time'], retain=sched['hourly']['retain'])
        schedules[schedname] = SnapSchedule(schedname, monthly, weekly, daily, hourly)

    # let's get to work!

    filesystems = config["filesystems"]

    # upload queue for queuing object uploads
    #uploadq = queue.Queue()

    # start the background uploader
    #upload_thread = threading.Thread(target=background_uploader, args=(uploadq,))
    #upload_thread.start()

    for fsname, snapname in replay_upload_intent_log():
        UploadSnapshot(cluster_obj, fsname, snapname)

    #logging.debug(f"upload_thread = {upload_thread}")

    while True:
        # go get a list of existing snaps in the cluster
        try:
            #snapshots = cluster_obj.call_api(method="snapshots_list",parms={})
            snapshots = snapd_call_api(cluster_obj, method="snapshots_list",parms={})
        except Exception as exc:
            logger.error(f"Error collecting snapshot list: {exc}")
            sys.exit(1)

        sorted_snaps = {}
        for snapshot in snapshots:
            fsname = snapshot["filesystem"]
            snapname = snapshot["name"]
            # make sure it's one of ours, not one made by the customer
            snap_list = snapname.split('.')
            if len(snap_list) != 2:     #first hint - schedule.time is the format; must be 2 here 
                continue
            if snap_list[0] not in ["monthly","weekly","daily","hourly"]:    # ours start with the schedule name
                continue

            # got one of ours, record it
            if fsname not in sorted_snaps:
                sorted_snaps[fsname] = []
            sorted_snaps[fsname].append(snapname)

        #print(json.dumps(sorted_snaps, indent=4))
        #created_snap = cluster_obj.call_api(method="snapshot_create",parms={"access_point":"myaccesspoint","file_system":"fs01","is_writable":False,"name":"snapname2"})
        #deleted_snap = cluster_obj.call_api(method="snapshot_delete",parms={"file_system":"fs01","name":"snapname2"})

        now = datetime.datetime.now()
        logger.info(f"current time is {now}")
        #now = datetime.datetime(2020,12,31,23,59,45) # for testing

        have_slept = False

        for fsname, filesystem in filesystems.items():
            schedname = filesystem["schedule"]
            schedule = schedules[schedname]
            next_snap = schedule.next_snap(now)

            logger.info(f"next snap for {fsname} will be a {schedule.name} at: {next_snap}")

            time_to_snap = next_snap.next_snaptime(now) - now

            # is there a snap to be done now?
            if time_to_snap < datetime.timedelta(minutes=1):
                if not have_slept:
                    logger.debug(f"less than 1 min to go to snapshot time! Sleeping until time to snap")
                    sleep_time = time_to_snap.total_seconds()
                    if sleep_time < 0:
                        logger.error(f"negative sleep time {sleep_time}. now={now}, next_snap={next_snap}")
                    time.sleep(sleep_time)
                have_slept = True

                # process snapshots
                next_snapname = next_snap.name + "." + next_snap.next_snaptime(now).strftime("%Y-%m-%d_%H%M")
                logger.debug(f"Next snapname is {next_snapname}")

                if next_snap.retain > 0:    # number of snaps to keep >0
                    # create a snap
                    logger.debug(f"snap {next_snapname} to be created on fs {fsname}")
                    try:
                        created_snap = cluster_obj.call_api(method="snapshot_create",parms={
                            "file_system":fsname,
                            "name":next_snapname,
                            "access_point":next_snapname,
                            "is_writable":False})
                        logger.info(f"snap {next_snapname} has been created on fs {fsname}")
                        # needs error-checking
                    except Exception as exc:
                        logger.error(f"error creating snapshot {next_snapname} on filesystem {fsname}: {exc}")

                    # upload it?
                    if filesystem['upload']:
                        # if upload is True, queue it for uploading
                        UploadSnapshot(cluster_obj, fsname, next_snapname)

                    # delete old snaps
                    logger.info(f"fs {fsname} should keep {next_snap.retain} {next_snap.name} snaps, and currently has {len(sorted_snaps[fsname])} of them")
                    if len(sorted_snaps[fsname]) > next_snap.retain:
                        # need to delete at least 1 snap - oldest first
                        logger.debug(f"cleaning up old snaps")
                        count = 0
                        for snap in sorted(sorted_snaps[fsname]):   # sorted so oldest come up first
                            logger.info(f"looking at snap {fsname}/{snap}")
                            count += 1
                            if count > next_snap.retain:
                                logger.info(f"snap {fsname}/{snap} being deleted")
                                try:
                                    deleted_snap = cluster_obj.call_api(method="snapshot_delete",parms={"file_system":fsname,"name":snap})
                                except Exception as exc:
                                    logger.error(f"error deleting snapshot {snap} from filesystem {fsname}: {exc}")


            # next - snapshot uploads
            #try:
                #uploaded_snap = cluster_obj.call_api(method="snapshot_upload",parms={"file_system":fsname,"name":snap})
            #except Exception as exc:
            #    logger.error(f"error uploading snapshot {snap} from filesystem {fsname}: {exc}")
        logger.debug(f"Nothing to do; sleeping 1 min")
        time.sleep(60)

#logging.debug(f"terminating")
#UploadSnapshot(uploadq, "WEKA_TERMINATE_THREAD", "WEKA_TERMINATE_THREAD")
