#!/usr/bin/env python3

# Weka Snapshot Management Daemon
# Vince Fleming
# vince@weka.io
#
# updated for new scheduling - Bruce Clagett

from operator import attrgetter, itemgetter
import sys
import math
import argparse
import platform
import time

import wekalib.exceptions
import yaml
import urllib3
import logging.handlers

import datetime
from datetime import timezone

import wekalib.signals as signals
import wekalib.wekacluster as wekacluster
import snapshots
import background

VERSION = "0.11.0"

# get the root logger, get snaptool logger
log = logging.getLogger()
log.setLevel(logging.INFO)  # to start
snap_log = logging.getLogger(__name__)  # snaptool log (a file, set below)


"""
# cluster.refresh_config() doesn't exist.   Don't use this function for now
# check with Vince.  Looks like refresh_config() was part of the wekacluster before
def cluster_call_api(cluster, method, parms):
    num_errors = 0
    while True:
        # api only fails under extreme circumstances
        try:
            api_return = cluster.call_api(method=method, parms=parms)
            return api_return
        # except APIException as exc:
        except Exception as exc:
            num_errors += 1
            log.error(
                f"Error {exc} while executing API method {method}; "
                f"attempting to re-establish communications (retry {num_errors})")
            if num_errors > 10:  # give up
                raise
            time.sleep(5)  # give the cluster some time to resolve it's issues
            cluster.refresh_config()  # will try to re-establish communications
"""

def now():
    return datetime.datetime.now()

def setup_logging_initial():
    snaptool_f_handler = logging.handlers.RotatingFileHandler("snaptool.log", maxBytes=10 * 1024 * 1024, backupCount=2)
    snaptool_f_handler.setFormatter(logging.Formatter('%(asctime)s %(levelname)s: %(message)s'))
    log.addHandler(snaptool_f_handler)

    syslog_format = "%(process)s:%(filename)s:%(lineno)s:%(funcName)s():%(levelname)s:%(message)s"
    console_format = "%(asctime)s:%(levelname)7s:%(filename)s:%(lineno)s:%(funcName)s():%(message)s"
    # create handler to log to syslog
    log.info(f"setting syslog on {platform.platform()}")
    if platform.platform()[:5] == "macOS":
        syslogaddr = "/var/run/syslog"
    else:
        syslogaddr = "/dev/log"
    syslog_handler = logging.handlers.SysLogHandler(syslogaddr)
    syslog_handler.setFormatter(logging.Formatter(syslog_format))

    # create handler to log to stderr
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(logging.Formatter(console_format))

    # add handlers to root logger
    if syslog_handler is not None:
        log.addHandler(syslog_handler)
    log.addHandler(console_handler)
    log.info("-------------------------Program (re)start initial---------------------------")

def set_logging_levels(snaptool_level, snapshots_level=logging.ERROR,
                       background_level=logging.ERROR, wekalib_level=logging.ERROR):
    log.info("-------------------------Setting new log levels-------------------------------")
    log.setLevel(snaptool_level)
    snap_log.setLevel(snaptool_level)

    urllib3.add_stderr_logger(level=logging.ERROR)

    logging.getLogger("wekalib.wekacluster").setLevel(wekalib_level)
    logging.getLogger("wekalib.wekaapi").setLevel(wekalib_level)
    logging.getLogger("wekalib.sthreads").setLevel(wekalib_level)
    logging.getLogger("wekalib.circular").setLevel(wekalib_level)
    logging.getLogger("background").setLevel(background_level)
    logging.getLogger("snapshots").setLevel(snapshots_level)

class ScheduleGroup(object):
    def __str__(self):
        return f"(Group {self.name}: {len(self.entries)} entries; filesystems: {self.filesystems})"

    def __init__(self, name):
        self.name = name
        self.entries = []
        self.filesystems = []
        self.sort_priority = 9999
        self.no_upload = True
        self.next_snap_time = datetime.datetime.max

    def log_pp(self, loggerobject, level):
        msg = f"Group {self.name} (next snap: {self.next_snap_time})"
        msg += f" for filesystems {self.filesystems}, upload: {not self.no_upload}"
        loggerobject.log(level, msg)
        for e in self.entries:
            loggerobject.log(level, f"   {e.name}:\t{e.nextsnap_dt}\t({str(e)})")


def config_syntax_error(args, message):
    logging.error(f"Error in file {args.configfile}: {message}")
    sys.exit(1)

def syntax_check_top_level(args, config):
    if 'filesystems' not in config:
        config_syntax_error(args, "'filesystems' key not found")
    if 'schedules' not in config:
        config_syntax_error(args, "'schedules' key not found")
    if len(config) != 2:
        config_syntax_error(args, "there must be 2 top level keys in {args.configfilename}: filesystems and schedules")

def config_parse(config, args):
    resultsdict = {}
    syntax_check_top_level(args, config)
    filesystems = config['filesystems']
    schedules = config['schedules']
    snap_log.debug(f"JSON filesystems: {filesystems}")
    snap_log.debug(f"JSON schedules: {schedules}")
    for schedname, schedspec in schedules.items():
        new_group = ScheduleGroup(schedname)
        resultsdict[schedname] = new_group
        if "every" in schedspec.keys():    # single schedule item without a sub-schedule name
            entry = snapshots.parse_schedule_entry(None, schedname, schedspec)
            new_group.entries.append(entry)
        else:
            for schedentryname, schedentryspec in schedspec.items():
                entry = snapshots.parse_schedule_entry(schedname, schedentryname, schedentryspec)
                new_group.entries.append(entry)
    for fs_name, fs_schedulegroups in filesystems.items():
        if isinstance(fs_schedulegroups, str):
            fs_schedulegroups = snapshots.commastring_to_list(fs_schedulegroups)
            filesystems[fs_name] = fs_schedulegroups
        snap_log.info(f"{fs_name}, {fs_schedulegroups}")
        for sched_name in fs_schedulegroups:
            if sched_name not in resultsdict.keys():
                config_syntax_error(args, f"Schedule {sched_name}, listed for filesystem {fs_name}, not found")
            else:
                resultsdict[sched_name].filesystems.append(fs_name)
    return resultsdict

def parse_snaptool_args():
    argparser = argparse.ArgumentParser(description="Weka Snapshot Management Daemon")
    argparser.add_argument("-c", "--configfile", dest='configfile', default="./snaptool.yml",
                           help="override ./snaptool.yml as config file")
    argparser.add_argument("-p", "--port", dest='port', default="13999", help="TCP port number to listen on")
    argparser.add_argument("clusterspec", default="localhost", help="Cluster specification.  <host>,<host>")
    argparser.add_argument("--auth-file", dest='authfile',
                           default="~/auth-token.json", help="Cluster authentication token file")
    argparser.add_argument("-v", "--verbosity", action="count", default=0, help="increase output verbosity")
    argparser.add_argument("--version", dest="version", default=False, action="store_true",
                           help="Display version number")
    args = argparser.parse_args()

    if args.version:
        snap_log.info(f"{sys.argv[0]} version {VERSION}")
        sys.exit(0)

    if args.verbosity == 0:
        loglevel = logging.ERROR
    elif args.verbosity == 1:
        loglevel = logging.WARNING
    elif args.verbosity == 2:
        loglevel = logging.INFO
    else:
        loglevel = logging.DEBUG

    return args, loglevel

def update_snaptimes_sort_and_clean(snapgrouplist, now_dt):
    unused_list = []
    snap_log.info(f"schedule groups before unused check: {len(snapgrouplist)}")
    for sg in snapgrouplist:
        if len(sg.filesystems) == 0:
            snap_log.warning(f"unused schedule {sg.name}, ignoring")
            unused_list.append(sg)
    for sg in unused_list:
        snapgrouplist.remove(sg)
    snap_log.info(f"schedule groups after unused check: {len(snapgrouplist)}")
    # update snaptimes in entries and in SnapGroups, and sort
    for sg in snapgrouplist:
        for entry in sg.entries:
            entry.calc_next_snaptime(now_dt)
        sg.entries.sort(key=attrgetter('nextsnap_dt', 'sort_priority', 'no_upload'))
        if len(sg.entries) > 0:
            sg.next_snap_time = sg.entries[0].nextsnap_dt
            sg.sort_priority = sg.entries[0].sort_priority
            sg.no_upload = sg.entries[0].no_upload
    snapgrouplist.sort(key=attrgetter('next_snap_time', 'sort_priority', 'no_upload'))

def log_snapgrouplist(snapgroup_list):
    for sg in snapgroup_list:
        sg.log_pp(snap_log, logging.DEBUG)

def get_snapgroups_for_snaptime(snapgroup_list, snaptime):
    result = []
    for item in snapgroup_list:
        if item.next_snap_time == snaptime:
            result.append(item)
        else:
            break
    return result

def get_snaps_dict_by_fs(snapgroups_for_nextsnap, next_snap_time):
    results = {}
    for sg in snapgroups_for_nextsnap:
        snap_log.info(f"   sg {sg.name}, sg.filesystems: {sg.filesystems}")
        for fs in sg.filesystems:
            if fs not in results:
                snap_log.info(f"        {sg.name} {sg.entries[0].name} will snap {fs} at {next_snap_time} ")
                results[fs] = sg.entries[0]
            else:
                snap_log.info(f"        conflicting snap of {fs} ignored")
    snap_log.info(f"get_snaps_dict_by_fs: {len(results)} entries; filesystems: {list(results.keys())}")
    return results

def next_snaps(parsed_schedules_dict):
    sg_list = list(parsed_schedules_dict.values())
    update_snaptimes_sort_and_clean(sg_list, now())
    log_snapgrouplist(sg_list)
    next_snap_time = sg_list[0].entries[0].nextsnap_dt    # because it's sorted this works
    snapgroups_for_nextsnap = get_snapgroups_for_snaptime(sg_list, next_snap_time)
    snap_log.info(f"snap groups for next snap: {len(snapgroups_for_nextsnap)} entries for {next_snap_time}")
    return next_snap_time, get_snaps_dict_by_fs(snapgroups_for_nextsnap, next_snap_time)

def get_snapshots(cluster):
    try:
        snapshot_list = cluster.call_api(method="snapshots_list", parms={})
    except Exception as exc:
        log.error(f"Error collecting snapshot list: {exc}")
        snapshot_list = []
        # sys.exit(1)
    return snapshot_list

def create_snapshot(cluster, fs, name, access_point_name):
    try:
        created_snap = cluster.call_api(method="snapshot_create", parms={
            "file_system": fs,
            "name": name,
            "access_point": access_point_name,
            "is_writable": False})
        log.info(f"snap {name} has been created on fs {fs}: {created_snap}")
    # needs error-checking
    except Exception as exc:
        log.error(f"Error creating snapshot {name} on filesystem {fs}: {exc}")

def get_fs_snaps(all_snaps, fs, schedname):
    # return snaps for fs that are named <schedname>.<something>, return them sorted by creation time
    log.info(f"Getting snaps specific to {fs} and {schedname}")
    snaps_for_fs = []
    for s in all_snaps:
        snap_name = s['name'].split('.')
        if s['filesystem'] == fs and len(snap_name) == 2 and snap_name[0] == schedname:
            snaps_for_fs.append(s)
    snaps_for_fs.sort(key=itemgetter('creationTime'))
    return snaps_for_fs

def delete_old_snaps(cluster, parsed_schedules_dict):
    # look at all defined schedule groups, not just last loop snaps
    # in case retentions have changed (for example, to 0)
    all_snaps = get_snapshots(cluster)
    sg_list = list(parsed_schedules_dict.values())
    for sg in sg_list:
        for entry in sg.entries:
            for fs in sg.filesystems:
                snaps = [s for s in all_snaps if s['name'].split(".")[0] == entry.name and s['filesystem'] == fs]
                snaps.sort(key=itemgetter('creationTime'))
                if len(snaps) > entry.retain:
                    num_to_delete = len(snaps) - entry.retain
                    snaps_to_delete = snaps[:num_to_delete]
                    for s in snaps_to_delete:
                        log.info(f"Queueing fs {fs}/{s['name']} for delete")
                        background.QueueOperation(cluster, fs, s['name'], "delete")


def create_new_snaps(cluster, snaps_taken_dict, next_snap_time):
    for fs, snap in snaps_taken_dict.items():
        access_point_name = next_snap_time.astimezone(timezone.utc).strftime(
            "@GMT-%Y.%m.%d-%H.%M.%S")  # windows format
        # don't need century, or date at all really, because snap creation time is used for delete, comparisons
        # date in the name is really for convenience in displays
        next_snap_name = snap.name + "." + next_snap_time.strftime("%y%m%d%H%M")
        log.info(f"next_snap_name:{next_snap_name} (len={len(next_snap_name)}), access_point_name:{access_point_name}")
        create_snapshot(cluster, fs, next_snap_name, access_point_name)

def check_cluster_connection(cluster):
    try:
        result = cluster.call_api(method='status', parms={})
    except wekalib.exceptions.APIError as exc:
        log.error(f"Cluster connection check failure: {cluster} - {exc}")
        return False
    log.debug(f"Cluster connected: {cluster} iostatus: {result['io_status']}")
    return True

def get_cluster_connection(clusterspec, authfile, cluster=None):
    if cluster and check_cluster_connection(cluster):
        return cluster
    else:
        cluster = wekacluster.WekaCluster(clusterspec, authfile)
        if check_cluster_connection(cluster):
            return cluster
        else:
            raise wekalib.exceptions.NewConnectionError(f"Couldn't connect to cluster {clusterspec}, {authfile}")

def refresh_cluster_connection(clusterspec, authfile, cluster):
    return get_cluster_connection(clusterspec, authfile, cluster)

def get_configfile_config(filename):
    with open(filename, 'r') as f:
        config = yaml.load(stream=f, Loader=yaml.BaseLoader)
    snap_log.debug(config)
    return config

def main():
    # handle signals (ie: ^C and such)
    setup_logging_initial()
    signals.signal_handling()
    args, loglevel = parse_snaptool_args()
    set_logging_levels(loglevel, snapshots_level=loglevel, background_level=logging.INFO)

    # tests
    run_tests = True
    if run_tests:
        snapshots.run_schedule_tests()

    config = get_configfile_config(args.configfile)
    schedules_dict = config_parse(config, args)

    log.info("Trying to create cluster connection...")
    cluster = get_cluster_connection(args.clusterspec, args.authfile)

    log.warning("Replaying background operation intent log...")
    background.intent_log.replay(cluster)

    # do this once at beginning so we don't wait for next snap time to clean up anything missed from previous runs
    # depending on timing, may end up with "already deleted" messages in logs
    delete_old_snaps(cluster, schedules_dict)

    reload_interval = 600
    force_reconnect_interval = 3600
    last_reload_time = time.time()
    last_reconnect_time = time.time()

    while True:
        if (time.time() - last_reload_time) > reload_interval:
            log.info(f"Reloading configuration file {args.configfile}")
            config = get_configfile_config(args.configfile)
            schedules_dict = config_parse(config, args)
            log.info(f"Confirming cluster connection...")
            if not check_cluster_connection(cluster):
                cluster = refresh_cluster_connection(args.clusterspec, args.authfile, cluster)
            last_reload_time = time.time()
        if (time.time() - last_reconnect_time) > force_reconnect_interval:
            log.info(f"Reconnecting to cluster due to force_reconnect interval {force_reconnect_interval}")
            cluster = get_cluster_connection(args.clusterspec, args.authfile)
            last_reconnect_time = time.time()

        next_snap_time, next_snaps_dict = next_snaps(schedules_dict)
        sleep_time = math.floor((next_snap_time - now()).total_seconds())
        fs_msg_str = f"{ {fs: str(s) for fs, s in next_snaps_dict.items()} }"
        if sleep_time <= 0:
            sleep_msg = f"No need to sleep, snap now ({next_snap_time}): {fs_msg_str}"
            sleep_time = 0
        else:
            sleep_msg = f"Sleep until {next_snap_time}, ({sleep_time} seconds), then snap: {fs_msg_str}"

        log.info(sleep_msg)
        time.sleep(sleep_time)

        create_new_snaps(cluster, next_snaps_dict, next_snap_time)
        delete_old_snaps(cluster, schedules_dict)

        # prevent same snap attempts from running again in the same minute
        sleep_time = 60 - math.floor((now() - next_snap_time).total_seconds())
        if sleep_time > 0:
            log.info(f"Sleeping for {sleep_time} seconds before next loop")
            time.sleep(sleep_time)


if __name__ == '__main__':
    main()
